{
    "layers": [
        8182,
        4096,
        1
    ],
    "act_func": "relu",
    "dropout": 0.5,
    "input_dropout": 0.2,
    "eta": 0.00001,
    "norm": "tanh"
}